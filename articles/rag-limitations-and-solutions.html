<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="A comprehensive analysis of RAG limitations, 12 critical failure points, and solutions including GraphRAG, Agentic Memory, and best practices for enterprise AI chatbots.">
    <meta name="keywords"
        content="RAG, GraphRAG, AI Chatbot, LLM, Vector Database, FAISS, Retrieval Augmented Generation, Agent Memory">
    <meta name="author" content="Kissbu Learns">

    <title>Beyond Simple RAG: Understanding Limitations and Building Better AI Chatbots | Kissbu Learns</title>

    <!-- Open Graph -->
    <meta property="og:title" content="Beyond Simple RAG: Understanding Limitations and Building Better AI Chatbots">
    <meta property="og:description"
        content="A comprehensive analysis of 12 critical failure points in RAG systems with solutions for enterprise knowledge bases.">
    <meta property="og:type" content="article">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=Fira+Code:wght@400;500&display=swap"
        rel="stylesheet">

    <!-- Styles -->
    <link rel="stylesheet" href="../css/styles.css">

    <!-- Favicon -->
    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ğŸ§ </text></svg>">
</head>

<body>
    <!-- Background Effects -->
    <div class="bg-gradient"></div>
    <div class="bg-grid"></div>
    <div class="bg-orb bg-orb-1"></div>
    <div class="bg-orb bg-orb-2"></div>

    <!-- Header -->
    <header class="site-header">
        <div class="container">
            <div class="header-content">
                <a href="../index.html" class="logo">
                    <div class="logo-icon">ğŸ§ </div>
                    <span>Kissbu Learns</span>
                </a>
                <nav class="nav-links">
                    <a href="../index.html">Home</a>
                    <a href="../index.html#articles">Articles</a>
                    <a href="../index.html#about">About</a>
                </nav>
            </div>
        </div>
    </header>

    <!-- Reading Progress -->
    <div class="reading-progress">
        <div class="reading-progress-bar"></div>
    </div>

    <!-- Article Header -->
    <header class="article-header">
        <div class="container content-wrapper">
            <div class="article-meta" style="justify-content: center; margin-bottom: 1rem;">
                <span class="article-tag">RAG</span>
                <span class="article-tag">AI Architecture</span>
                <span class="article-tag">Best Practices</span>
            </div>
            <h1>Beyond Simple RAG: Understanding Limitations and Building Better AI Chatbots</h1>
            <div class="article-header-meta">
                <span>ğŸ“… December 8, 2024</span>
                <span>â±ï¸ 20 min read</span>
                <span>âœï¸ Kissbu Learns</span>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main class="article-content">
        <div class="container">
            <div class="content-wrapper">

                <!-- Table of Contents -->
                <nav class="toc">
                    <h4>ğŸ“‘ Table of Contents</h4>
                    <ul class="toc-list">
                        <!-- Auto-generated by JS -->
                    </ul>
                </nav>

                <!-- Introduction -->
                <h2 id="introduction">Introduction</h2>
                <p>
                    Across enterprises, teams are increasingly building <strong>RAG-based chatbots and AI
                        agents</strong>
                    to leverage their internal knowledge basesâ€”product documentation, JIRA confluence articles, known
                    issues, and historical case resolutions. The promise is compelling: give your LLM access to your
                    organization's knowledge without expensive fine-tuning.
                </p>

                <p>
                    However, as more teams deploy these systems into production, a pattern emerges: <strong>simple RAG
                        implementations often fall short</strong>. The chatbot retrieves irrelevant documents, misses
                    crucial
                    context, hallucinates answers, or simply takes too long to respond.
                </p>

                <div class="callout callout-warning">
                    <strong>The Reality Check:</strong> Research from Deakin University identified 7 distinct failure
                    points in RAG systems, while industry practitioners at Databricks documented 5 major challenges.
                    Understanding these limitations is the first step to building robust AI solutions.
                </div>

                <p>
                    This comprehensive guide consolidates research from academic papers and industry leaders to help
                    you understand <em>why</em> simple RAG fails and <em>what</em> you can do about it.
                </p>

                <!-- What is RAG -->
                <h2 id="what-is-rag">What is RAG? A Quick Primer</h2>

                <p>
                    <strong>Retrieval-Augmented Generation (RAG)</strong> is a GenAI technique that incorporates
                    relevant
                    data as context to a Large Language Model (LLM) without requiring fine-tuning. Instead of relying
                    solely on the model's pre-trained knowledge, RAG dynamically retrieves information from an external
                    knowledge base to generate more accurate, up-to-date responses.
                </p>

                <h3 id="rag-architecture">The RAG Pipeline</h3>

                <p>A typical RAG system operates in three main phases:</p>

                <div class="diagram-container">
                    <pre
                        style="background: transparent; border: none; text-align: center; font-family: var(--font-mono); color: var(--text-secondary);">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG ARCHITECTURE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   ğŸ“„ Documents    â”€â”€â–¶  ğŸ”¢ Embeddings   â”€â”€â–¶  ğŸ“¦ Vector Database       â”‚
â”‚   (PDF, Docs)         (Chunking)            (FAISS, Pinecone)       â”‚
â”‚                                                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚                                                                      â”‚
â”‚   ğŸ’¬ User Query   â”€â”€â–¶  ğŸ” Similarity    â”€â”€â–¶  ğŸ“‹ Retrieved Docs      â”‚
â”‚                         Search               (Top-K Results)        â”‚
â”‚                                                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚                                                                      â”‚
â”‚   ğŸ“‹ Context      â”€â”€â–¶  ğŸ¤– LLM           â”€â”€â–¶  ğŸ’¡ Generated Answer    â”‚
â”‚   + Query              (GPT, Claude)                                â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          </pre>
                    <div class="diagram-caption">Figure 1: Simplified RAG Architecture Flow</div>
                </div>

                <ol>
                    <li><strong>Indexing Phase:</strong> Source documents are chunked, converted to embeddings, and
                        stored in a vector database</li>
                    <li><strong>Retrieval Phase:</strong> User queries are embedded and matched against stored documents
                        using similarity search</li>
                    <li><strong>Generation Phase:</strong> Retrieved documents become context for the LLM to generate an
                        answer</li>
                </ol>

                <blockquote>
                    "RAG shines in use cases where the relevant information is continuously evolving (e.g., customer
                    order data),
                    or where the model encounters unfamiliar data outside its training set."
                    <br><br>
                    â€” <em>Databricks Technical Blog</em>
                </blockquote>

                <!-- 12 Failure Points -->
                <h2 id="failure-points">The 12 Critical Failure Points</h2>

                <p>
                    Drawing from research by Deakin University's software engineering team and Databricks practitioners,
                    we've consolidated <strong>12 critical failure points</strong> that plague RAG implementations.
                    Understanding these is essential before deploying any RAG-based chatbot.
                </p>

                <h3 id="retrieval-failures">Retrieval-Related Failures</h3>

                <div class="fp-grid">
                    <div class="fp-card">
                        <div class="fp-number">1</div>
                        <h4>Missing Content</h4>
                        <p>
                            The question cannot be answered from the available documents. Ideally, the system should say
                            "I don't know," but often provides a fabricated answer instead.
                        </p>
                        <div class="fp-source">ğŸ“š Source: arXiv Paper (2401.05856)</div>
                    </div>

                    <div class="fp-card">
                        <div class="fp-number">2</div>
                        <h4>Missed Top Ranked Documents</h4>
                        <p>
                            The answer exists in the knowledge base but didn't rank highly enough in the Top-K results
                            to be returned to the LLM.
                        </p>
                        <div class="fp-source">ğŸ“š Source: arXiv Paper (2401.05856)</div>
                    </div>

                    <div class="fp-card">
                        <div class="fp-number">3</div>
                        <h4>Irrelevant Retrievals</h4>
                        <p>
                            The retriever returns documents that don't match the query intent. This often stems from
                            poor chunking strategies or mismatched embedding models.
                        </p>
                        <div class="fp-source">ğŸ“š Source: Databricks Community</div>
                    </div>

                    <div class="fp-card">
                        <div class="fp-number">4</div>
                        <h4>Query-Document Mismatch</h4>
                        <p>
                            RAG retrieves documents based on embedding similarity. If the query uses uncommon wording,
                            relevant documents may be missed entirely.
                        </p>
                        <div class="fp-source">ğŸ“š Source: HashStudioz</div>
                    </div>
                </div>

                <h3 id="context-failures">Context & Extraction Failures</h3>

                <div class="fp-grid">
                    <div class="fp-card">
                        <div class="fp-number">5</div>
                        <h4>Not in Context</h4>
                        <p>
                            Documents with the answer were retrieved but didn't make it into the prompt due to
                            consolidation strategies or token limits.
                        </p>
                        <div class="fp-source">ğŸ“š Source: arXiv Paper (2401.05856)</div>
                    </div>

                    <div class="fp-card">
                        <div class="fp-number">6</div>
                        <h4>Not Extracted</h4>
                        <p>
                            The answer is present in the context, but the LLM failed to extract it correctly.
                            Often caused by noise or contradicting information.
                        </p>
                        <div class="fp-source">ğŸ“š Source: arXiv Paper (2401.05856)</div>
                    </div>

                    <div class="fp-card">
                        <div class="fp-number">7</div>
                        <h4>Context Window Limits</h4>
                        <p>
                            LLMs struggle with lengthy prompts. Research shows models may ignore or
                            poorly process information based on its position in the prompt.
                        </p>
                        <div class="fp-source">ğŸ“š Source: Databricks Community</div>
                    </div>

                    <div class="fp-card">
                        <div class="fp-number">8</div>
                        <h4>Contradicting Sources</h4>
                        <p>
                            When retrieved documents contain conflicting information, the LLM may generate
                            incorrect answers or become confused.
                        </p>
                        <div class="fp-source">ğŸ“š Source: Databricks Community</div>
                    </div>
                </div>

                <h3 id="output-failures">Output Quality Failures</h3>

                <div class="fp-grid">
                    <div class="fp-card">
                        <div class="fp-number">9</div>
                        <h4>Wrong Format</h4>
                        <p>
                            The LLM ignores format instructions. When asked for a table or list,
                            it may return prose instead.
                        </p>
                        <div class="fp-source">ğŸ“š Source: arXiv Paper (2401.05856)</div>
                    </div>

                    <div class="fp-card">
                        <div class="fp-number">10</div>
                        <h4>Incorrect Specificity</h4>
                        <p>
                            Answers are either too general or too specific for the user's needs.
                            Common when users ask vague questions.
                        </p>
                        <div class="fp-source">ğŸ“š Source: arXiv Paper (2401.05856)</div>
                    </div>

                    <div class="fp-card">
                        <div class="fp-number">11</div>
                        <h4>Incomplete Answers</h4>
                        <p>
                            The response misses information that was available in the context.
                            Multi-part questions are particularly vulnerable.
                        </p>
                        <div class="fp-source">ğŸ“š Source: arXiv Paper (2401.05856)</div>
                    </div>

                    <div class="fp-card">
                        <div class="fp-number">12</div>
                        <h4>Hallucinations</h4>
                        <p>
                            The LLM generates plausible-sounding but factually incorrect information,
                            even when relevant context is provided.
                        </p>
                        <div class="fp-source">ğŸ“š Source: Databricks Community</div>
                    </div>
                </div>

                <h3 id="operational-failures">Operational Challenges</h3>

                <p>Beyond the core pipeline failures, teams face significant operational challenges:</p>

                <table>
                    <thead>
                        <tr>
                            <th>Challenge</th>
                            <th>Description</th>
                            <th>Impact</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Performance</strong></td>
                            <td>Slow embedding, retrieval, or LLM inference</td>
                            <td>Poor user experience, timeouts</td>
                        </tr>
                        <tr>
                            <td><strong>Data Management</strong></td>
                            <td>Difficulty updating/deleting vectors</td>
                            <td>Stale information, inconsistencies</td>
                        </tr>
                        <tr>
                            <td><strong>Security</strong></td>
                            <td>Sensitive data exposure in retrievals</td>
                            <td>Compliance violations, data leaks</td>
                        </tr>
                        <tr>
                            <td><strong>Monitoring</strong></td>
                            <td>Lack of visibility into system behavior</td>
                            <td>Undetected degradation</td>
                        </tr>
                    </tbody>
                </table>

                <!-- RAG vs GraphRAG -->
                <h2 id="rag-vs-graphrag">Traditional RAG vs GraphRAG</h2>

                <p>
                    One fundamental limitation of traditional RAG is its inability to understand
                    <strong>relationships between concepts</strong>. It treats each document independently,
                    making it ineffective for queries requiring logical connections across multiple data points.
                </p>

                <h3 id="graphrag-solution">Enter GraphRAG</h3>

                <p>
                    <strong>Graph Retrieval-Augmented Generation (GraphRAG)</strong> replaces unstructured
                    text documents with a structured knowledge graph. This enables:
                </p>

                <ul>
                    <li><strong>Entity Recognition:</strong> Identifying people, organizations, events, and their
                        properties</li>
                    <li><strong>Relationship Mapping:</strong> Understanding how entities connect</li>
                    <li><strong>Multi-hop Reasoning:</strong> Following chains of relationships to answer complex
                        queries</li>
                </ul>

                <div class="diagram-container">
                    <pre
                        style="background: transparent; border: none; text-align: center; font-family: var(--font-mono); color: var(--text-secondary);">
Traditional RAG:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"Elon Musk founded Tesla"  â†’  [Vector]  â†’  Cosine Similarity Search

GraphRAG:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Elon Musk â”‚â”€foundedâ”€â”‚  Tesla   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚
      founded              produces
         â–¼                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ SpaceX   â”‚         â”‚Model 3   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          </pre>
                    <div class="diagram-caption">Figure 2: Traditional RAG vs GraphRAG Structure</div>
                </div>

                <h3 id="multi-hop-example">Multi-Hop Reasoning Example</h3>

                <div class="callout callout-info">
                    <strong>Query:</strong> "Which companies founded by Elon Musk produce electric vehicles?"
                    <br><br>
                    <strong>Traditional RAG:</strong> May return documents about Elon Musk OR electric vehicles,
                    but struggle to connect the relationship chain.
                    <br><br>
                    <strong>GraphRAG:</strong> Traverses: Elon Musk â†’ (founded) â†’ [Tesla, SpaceX, ...] â†’
                    (produces) â†’ [Electric Vehicles] âœ“
                </div>

                <h3 id="comparison-table">Comparison: RAG vs GraphRAG</h3>

                <table>
                    <thead>
                        <tr>
                            <th>Aspect</th>
                            <th>Traditional RAG</th>
                            <th>GraphRAG</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Data Structure</strong></td>
                            <td>Unstructured text chunks</td>
                            <td>Structured knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>Retrieval Method</strong></td>
                            <td>Vector similarity search</td>
                            <td>Graph traversal + reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>Multi-hop Queries</strong></td>
                            <td>âŒ Limited</td>
                            <td>âœ… Native support</td>
                        </tr>
                        <tr>
                            <td><strong>Relationship Awareness</strong></td>
                            <td>âŒ No explicit modeling</td>
                            <td>âœ… Explicit edges</td>
                        </tr>
                        <tr>
                            <td><strong>Setup Complexity</strong></td>
                            <td>Lower</td>
                            <td>Higher (graph construction)</td>
                        </tr>
                        <tr>
                            <td><strong>Compute Requirements</strong></td>
                            <td>Lower</td>
                            <td>Higher (graph reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>Best For</strong></td>
                            <td>Simple Q&A, document search</td>
                            <td>Complex reasoning, relationship queries</td>
                        </tr>
                    </tbody>
                </table>

                <!-- RAG vs Agent Memory -->
                <h2 id="rag-vs-memory">RAG vs Agent Memory</h2>

                <p>
                    Another critical limitation, highlighted by <strong>Letta</strong>, is that
                    <em>RAG is not true memory</em>. While often used to provide conversation history
                    to AI agents, traditional RAG falls short in fundamental ways.
                </p>

                <h3 id="single-step-problem">Problem 1: RAG is Single-Step</h3>

                <p>
                    RAG gives the LLM "one shot" at retrieving relevant data and generating a response.
                    Consider this analogy from Letta:
                </p>

                <blockquote>
                    "Imagine asking students to write a book report. A RAG approach would shred the book
                    into pieces, select the top 10 most relevant shreds based on the assignment guidelines,
                    and ask students to write based on those fragments alone. They have no context for
                    the selected pieces, making it hard to form a coherent summary."
                    <br><br>
                    â€” <em>Letta Blog: RAG is not Agent Memory</em>
                </blockquote>

                <h3 id="reactive-problem">Problem 2: RAG is Purely Reactive</h3>

                <p>
                    If a user says "Today is my birthday," a RAG agent searches for "birthday" in the
                    vector database. But what about related personal information mentioned in past conversations?
                </p>

                <ul>
                    <li>User's favorite color mentioned 3 conversations ago</li>
                    <li>Their Star Wars movie preference from last week</li>
                    <li>The party theme they discussed last month</li>
                </ul>

                <p>
                    These don't match semantically with "birthday," so they're never retrieved.
                    A true memory system would proactively connect these dots.
                </p>

                <h3 id="agentic-rag">The Solution: Agentic RAG</h3>

                <p>
                    <strong>Agentic RAG</strong> combines multi-step reasoning with tools, allowing the
                    agent to iteratively explore data, maintain state, and build comprehensive understanding:
                </p>

                <div class="solutions-grid">
                    <div class="solution-card">
                        <div class="solution-icon">ğŸ“–</div>
                        <h4>Iterative Reading</h4>
                        <p>Read and summarize in chunks, updating understanding progressively</p>
                    </div>

                    <div class="solution-card">
                        <div class="solution-icon">ğŸ”</div>
                        <h4>Search Tools</h4>
                        <p>Multiple search strategies: keyword, semantic, and filtered queries</p>
                    </div>

                    <div class="solution-card">
                        <div class="solution-icon">ğŸ“</div>
                        <h4>State Management</h4>
                        <p>Maintain memory blocks that persist across conversations</p>
                    </div>

                    <div class="solution-card">
                        <div class="solution-icon">ğŸ¯</div>
                        <h4>Proactive Recall</h4>
                        <p>Distill and organize important information for future reference</p>
                    </div>
                </div>

                <!-- Solutions & Best Practices -->
                <h2 id="solutions">Solutions & Best Practices</h2>

                <p>
                    Based on our research synthesis, here are the key solutions to address RAG limitations:
                </p>

                <h3 id="chunking-strategies">1. Smart Chunking Strategies</h3>

                <p>
                    The quality of chunking directly affects retrieval accuracy. Consider:
                </p>

                <ul>
                    <li><strong>Semantic Chunking:</strong> Use the text's meaning to determine chunk boundaries, not
                        just character limits</li>
                    <li><strong>Overlap:</strong> Include overlapping content between chunks to preserve context</li>
                    <li><strong>Hierarchical:</strong> Create parent-child relationships for long documents</li>
                    <li><strong>Metadata Enrichment:</strong> Attach source, date, and category metadata to each chunk
                    </li>
                </ul>

                <pre>
<div class="code-header"><span class="code-lang">Python</span><button class="copy-btn">Copy</button></div><code># Example: Semantic chunking with LangChain
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " "],
    length_function=len
)

chunks = splitter.split_documents(documents)</code></pre>

                <h3 id="hybrid-search">2. Hybrid Search (Vector + Keyword)</h3>

                <p>
                    Pure vector search can miss exact matches. Combine with BM25 for better coverage:
                </p>

                <ul>
                    <li><strong>Vector Search:</strong> Captures semantic similarity</li>
                    <li><strong>BM25/TF-IDF:</strong> Handles exact keyword matches, acronyms, technical terms</li>
                    <li><strong>Fusion:</strong> Use Reciprocal Rank Fusion (RRF) to combine results</li>
                </ul>

                <h3 id="reranking">3. Re-ranking Layer</h3>

                <p>
                    Add a cross-encoder reranker to improve precision on retrieved results:
                </p>

                <pre>
<div class="code-header"><span class="code-lang">Python</span><button class="copy-btn">Copy</button></div><code># Example: Using a reranker
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# Get initial top-K results
candidates = vector_store.similarity_search(query, k=20)

# Rerank with cross-encoder
pairs = [(query, doc.page_content) for doc in candidates]
scores = reranker.predict(pairs)

# Sort by reranker scores
reranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
top_results = [doc for doc, score in reranked[:5]]</code></pre>

                <h3 id="semantic-caching">4. Semantic Caching</h3>

                <p>
                    Cache similar queries to reduce latency and costs:
                </p>

                <ul>
                    <li>If a semantically similar query was recently answered, return cached response</li>
                    <li>Reduces LLM API calls for common questions</li>
                    <li>MongoDB Atlas, Redis, or custom implementations work well</li>
                </ul>

                <h3 id="memory-layer">5. Conversation Memory</h3>

                <p>
                    Implement proper memory management for chatbots:
                </p>

                <ul>
                    <li><strong>Short-term:</strong> Recent messages in context window</li>
                    <li><strong>Long-term:</strong> Summarized conversation history</li>
                    <li><strong>Entity Memory:</strong> Track mentioned entities and their attributes</li>
                    <li><strong>User Preferences:</strong> Store personalization data</li>
                </ul>

                <h3 id="guardrails">6. Guardrails & Monitoring</h3>

                <p>
                    Implement quality controls:
                </p>

                <ul>
                    <li><strong>Input Guardrails:</strong> Validate and sanitize user queries</li>
                    <li><strong>Output Guardrails:</strong> Check for hallucinations, toxic content, PII exposure</li>
                    <li><strong>Relevance Scoring:</strong> Verify retrieved documents match intent</li>
                    <li><strong>LLM-as-Judge:</strong> Use an LLM to evaluate answer quality</li>
                    <li><strong>Human Feedback:</strong> Collect thumbs up/down for continuous improvement</li>
                </ul>

                <!-- Recommendations -->
                <h2 id="recommendations">My Recommendations for Your Use Case</h2>

                <p>
                    For teams building RAG chatbots with <strong>product knowledge bases, JIRA/Confluence articles,
                        known issues, and historical cases</strong>, here's my recommended architecture:
                </p>

                <h3 id="recommended-architecture">Recommended Architecture</h3>

                <div class="diagram-container">
                    <pre
                        style="background: transparent; border: none; text-align: center; font-family: var(--font-mono); color: var(--text-secondary); font-size: 0.85rem;">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRODUCTION RAG ARCHITECTURE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ User Query  â”‚â”€â”€â”€â”€â–¶â”‚  QUERY PROCESSING LAYER                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â€¢ Query expansion & rewriting                    â”‚  â”‚
â”‚                      â”‚  â€¢ Intent classification                          â”‚  â”‚
â”‚                      â”‚  â€¢ Cache lookup (semantic)                        â”‚  â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                          â”‚                                   â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                      â”‚  HYBRID RETRIEVAL LAYER                           â”‚  â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚       â”‚              â”‚  â”‚ FAISS       â”‚    â”‚ BM25        â”‚              â”‚  â”‚
â”‚       â”‚              â”‚  â”‚ (Semantic)  â”‚    â”‚ (Keyword)   â”‚              â”‚  â”‚
â”‚       â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚       â”‚              â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚  â”‚
â”‚   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”          â”‚              RRF Fusion                          â”‚  â”‚
â”‚   â”‚Memory â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   â”‚ Store â”‚                              â”‚                                   â”‚
â”‚   â”‚       â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚â€¢ Chat â”‚          â”‚  RE-RANKING LAYER                                 â”‚  â”‚
â”‚   â”‚  Hist â”‚          â”‚  â€¢ Cross-encoder scoring                          â”‚  â”‚
â”‚   â”‚â€¢ User â”‚          â”‚  â€¢ Metadata filtering (dates, categories)         â”‚  â”‚
â”‚   â”‚  Prefsâ”‚          â”‚  â€¢ Access control check                           â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                          â”‚                                   â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                      â”‚  GENERATION LAYER                                 â”‚  â”‚
â”‚                      â”‚  â€¢ Context assembly (memory + retrieved docs)     â”‚  â”‚
â”‚                      â”‚  â€¢ LLM inference                                  â”‚  â”‚
â”‚                      â”‚  â€¢ Source attribution                             â”‚  â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                          â”‚                                   â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                      â”‚  GUARDRAILS & POST-PROCESSING                     â”‚  â”‚
â”‚                      â”‚  â€¢ Hallucination detection                        â”‚  â”‚
â”‚                      â”‚  â€¢ PII masking                                    â”‚  â”‚
â”‚                      â”‚  â€¢ Response quality check                         â”‚  â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                          â”‚                                   â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                      â”‚       Response           â”‚â”€â”€â”€â”€â–¶ Cache Update         â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          </pre>
                    <div class="diagram-caption">Figure 3: Recommended Production RAG Architecture</div>
                </div>

                <h3 id="tech-stack">Recommended Technology Stack</h3>

                <table>
                    <thead>
                        <tr>
                            <th>Component</th>
                            <th>Recommendation</th>
                            <th>Rationale</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Vector DB</strong></td>
                            <td>FAISS (as planned) or Qdrant</td>
                            <td>FAISS is fast and works well for your scale. Qdrant adds filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>Embeddings</strong></td>
                            <td>OpenAI text-embedding-3-small or BGE</td>
                            <td>Good balance of quality and cost. BGE is open-source alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>Keyword Search</strong></td>
                            <td>Elasticsearch or BM25</td>
                            <td>Essential for technical terms, error codes, JIRA IDs.</td>
                        </tr>
                        <tr>
                            <td><strong>Reranker</strong></td>
                            <td>Cohere Rerank or cross-encoder</td>
                            <td>Significant precision improvement for top results.</td>
                        </tr>
                        <tr>
                            <td><strong>Framework</strong></td>
                            <td>LangChain or LlamaIndex</td>
                            <td>Proven frameworks with good component support.</td>
                        </tr>
                        <tr>
                            <td><strong>Memory</strong></td>
                            <td>MongoDB or Redis</td>
                            <td>Store chat history, user preferences, semantic cache.</td>
                        </tr>
                        <tr>
                            <td><strong>LLM</strong></td>
                            <td>GPT-4 / Claude / Gemini</td>
                            <td>Choose based on cost, latency, and enterprise agreements.</td>
                        </tr>
                    </tbody>
                </table>

                <h3 id="implementation-priorities">Implementation Priorities</h3>

                <p>
                    If starting from scratch, implement in this order:
                </p>

                <ol>
                    <li><strong>Week 1-2:</strong> Basic RAG with FAISS + semantic chunking</li>
                    <li><strong>Week 3-4:</strong> Add BM25 hybrid search + RRF fusion</li>
                    <li><strong>Week 5-6:</strong> Implement reranker + metadata filtering</li>
                    <li><strong>Week 7-8:</strong> Add conversation memory + semantic caching</li>
                    <li><strong>Week 9-10:</strong> Guardrails, monitoring, evaluation framework</li>
                    <li><strong>Future:</strong> Consider GraphRAG for relationship-heavy use cases</li>
                </ol>

                <div class="callout callout-success">
                    <strong>Key Insight:</strong> Don't try to implement everything at once. Start with a
                    solid basic RAG, measure its performance, and iteratively add components based on
                    observed failure patterns.
                </div>

                <!-- Conclusion -->
                <h2 id="conclusion">Conclusion</h2>

                <p>
                    Simple RAG implementations are a great starting point, but production systems require
                    thoughtful engineering to address the 12 failure points we've discussed. The key takeaways are:
                </p>

                <ul>
                    <li><strong>Understand your failure modes:</strong> Monitor and categorize where your RAG fails</li>
                    <li><strong>Hybrid search is essential:</strong> Vector + keyword search catches more relevant
                        content</li>
                    <li><strong>Reranking matters:</strong> A good reranker significantly improves precision</li>
                    <li><strong>Memory is not retrieval:</strong> True conversational AI needs proper memory management
                    </li>
                    <li><strong>GraphRAG for relationships:</strong> When queries require multi-hop reasoning, consider
                        knowledge graphs</li>
                    <li><strong>Iterate based on data:</strong> Build evaluation frameworks and improve systematically
                    </li>
                </ul>

                <p>
                    Building robust AI chatbots for enterprise knowledge bases is an iterative journey.
                    Start simple, measure obsessively, and add complexity where it solves real problems.
                </p>

                <!-- References -->
                <div class="references">
                    <h2 id="references">References & Citations</h2>

                    <ol class="reference-list">
                        <li>
                            <strong>Barnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., & Abdelrazek, M.</strong>
                            (2024).
                            "Seven Failure Points When Engineering a Retrieval Augmented Generation System."
                            <em>arXiv preprint arXiv:2401.05856</em>.
                            <a href="https://arxiv.org/html/2401.05856v1"
                                target="_blank">https://arxiv.org/html/2401.05856v1</a>
                        </li>
                        <li>
                            <strong>Rachidi, L. & Zervou, M.</strong> (2024).
                            "Five Things That Can Go Wrong When Building RAG Applications."
                            <em>Databricks Community Technical Blog</em>.
                            <a href="https://community.databricks.com/t5/technical-blog/five-things-that-can-go-wrong-when-building-rag-applications/ba-p/67078"
                                target="_blank">Databricks Community</a>
                        </li>
                        <li>
                            <strong>HashStudioz.</strong> (2024).
                            "Difference Between RAG and Graph RAG: A Technical Perspective."
                            <a href="https://www.hashstudioz.com/blog/difference-between-rag-and-graph-rag-a-technical-perspective/"
                                target="_blank">HashStudioz Blog</a>
                        </li>
                        <li>
                            <strong>Letta.</strong> (2024).
                            "RAG is not Agent Memory."
                            <a href="https://www.letta.com/blog/rag-vs-agent-memory" target="_blank">Letta Blog</a>
                        </li>
                        <li>
                            <strong>MongoDB.</strong> (2024).
                            "Add Memory and Semantic Caching with LangChain and MongoDB."
                            <a href="https://www.mongodb.com/docs/atlas/ai-integrations/langchain/memory-semantic-cache/"
                                target="_blank">MongoDB Documentation</a>
                        </li>
                        <li>
                            <strong>Liu, N. F., et al.</strong> (2023).
                            "Lost in the Middle: How Language Models Use Long Contexts."
                            <em>arXiv preprint arXiv:2307.03172</em>.
                        </li>
                    </ol>
                </div>

            </div>
        </div>
    </main>

    <!-- Footer -->
    <footer class="site-footer">
        <div class="container">
            <div class="footer-links">
                <a href="../index.html">Home</a>
                <a href="../index.html#articles">Articles</a>
                <a href="../index.html#about">About</a>
            </div>
            <p>Â© 2024 Kissbu Learns. Exploring AI, one article at a time.</p>
            <p style="margin-top: 0.5rem; font-size: 0.8rem;">
                Built with ğŸ’œ for the AI community
            </p>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../js/main.js"></script>
</body>

</html>